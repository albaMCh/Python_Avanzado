{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albaMCh/Python_Avanzado/blob/main/Proyecto_final_Python_Avanzado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto final Python Avanzado\n",
        "\n",
        "En este momento ya tienes unos conocimientos de programación en Python muy sólidos. Vamos a ponerlos a prueba preparando un  Modelo de bolsa de palabras (Bag Of Words, BoW).\n",
        "\n",
        "El modelo BagOfWords es un método que se utiliza en el procesado del lenguaje para representar documentos ignorando el orden de las palabras. Es decir, se reduce el contenido del documento a unas palabras clave y la frecuencia con la que aparece cada una de ellas, lo que nos permite convertir textos a una representación numérica (mucho más eficiente para trabajar con esos datos). \n",
        "\n",
        "Por ejemplo las frases:\n",
        "\n",
        "      \"A Carlos le gustan los perros. A Juan también le gustan los perros, pero prefiere gatos.\"\n",
        "\n",
        "      'A los gatos no le gustan los perros'\n",
        "\n",
        "También se pueden representar como la lista de sus palabras:\n",
        "\n",
        "      ['A', 'Carlos', 'le', 'gustan', 'los', 'perros.', 'A', 'Juan', 'también', 'le', 'gustan', 'los', 'perros,', 'pero', 'prefiere', 'gatos.']\n",
        "\n",
        "      ['A', 'los', 'gatos', 'no', 'le', 'gustan', 'los', 'perros.']\n",
        "\n",
        "E incluso, eliminando las palabras duplicadas y mostrando cuantas veces aparece cada palabra en la frase.\n",
        "\n",
        "      {'A': 2, 'Carlos': 1, 'le': 2, 'gustan': 2, 'los': 2, 'perros': 2, 'Juan': 1, 'también': 1, 'pero': 1, 'prefiere': 1, 'gatos': 1}\n",
        "\n",
        "      {'A': 1, 'los': 2, 'gatos': 1, 'no': 1, 'le': 1, 'gustan': 1, 'perros': 1}\n",
        "\n",
        "Esto permite, por ejemplo, clasificar documentos por que mencionen un tema y la importancia que tiene ese tema dentro del documento al tener en cuenta la frecuencia que aparezcan palabras relacionadas con ese tema. No aparecerá las mismas veces la cadena 'Inteligencia Artificial' en una publicación de un paper de Machine learning, que en la review de un nuevo videojuego o que en un cuento infantil.\n",
        "\n",
        "\n",
        "\n",
        "Para crear un modelo de BoW hay que seguir 4 pasos:\n",
        "\n",
        "* Limpiar el texto.\n",
        "\n",
        "* Tokenizar.\n",
        "\n",
        "* Generar el vocabulario.\n",
        "\n",
        "* Generar los vectores.\n",
        "\n",
        "Sabemos que es mucha información nueva de golpe, pero tranquilo, iremos paso a paso creando el modelo BoW y comprobando que funcione correctamente en cada paso.\n",
        "\n"
      ],
      "metadata": {
        "id": "SSGH1H7Ekqkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1. Limpiar el texto.\n",
        "\n",
        "Para poder convertir el texto a su representación gráfica es necesario realizar primero una tarea de limpieza y homogeneización.\n",
        "\n",
        "Si te fijas en el ejemplo del enunciado en el segundo paso, donde se representan las frases como la lista de sus palabras, para la primera frase aparece como elementos de la lista 'perros,' y 'perros.'. Si intentasemos pasar, tal cual está, del segundo paso al tercero, el diccionario con las frecuencias de aparición de cada palabra, los reconocería como elementos distintos, y no mostraría la frecuencia real.\n",
        "\n",
        "Esto también pasaría con palabras que estén en mayúscula, capitalizado o minúscula.\n",
        "\n",
        "Por ello, el primer paso para crear el modelo BoW será crear una clase, llamada ClasificacionDocumentos que únicamente contenga el método limpiar_texto, que reciba como parámetro una cadena de texto y devuelva el mismo texto en minúscula y eliminando todo lo que no sean palabras, números, espacios y los signos '$, €, @ y %'.\n",
        "\n",
        "**Pista:** puedes usar una expresión regular para sustituir todo lo que no sean palabras y números y añadir las restricciones para espacios y los 4 sígnos especificados.\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "* Entrada\n",
        "      texto_1='A Carlos le gustan los perros. A Juan también le gustan los perros, pero prefiere gatos.'\n",
        "      limpiar_texto(texto_1)\n",
        "\n",
        "* Salida\n",
        "\n",
        "      'a carlos le gustan los perros a juan también le gustan los perros pero prefiere gatos'\n",
        "\n",
        "Ejemplo 2\n",
        "\n",
        "* Entrada\n",
        "      texto_2='A C@rlos le gustan los p€rr0s. A Ju@n también le gustan los p€rr0s, ¡¿pero prefiere gatos?!.'\n",
        "      limpiar_texto(texto_2)\n",
        "\n",
        "* Salida\n",
        "\n",
        "      'a c@rlos le gustan los p€rr0s a ju@n también le gustan los p€rr0s pero prefiere gatos'\n"
      ],
      "metadata": {
        "id": "NnZZOpjo0CUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea la función limpiar texto\n",
        "import re\n",
        "\n",
        "class ClasificacionDocumentos:\n",
        "  \n",
        "  def limpiar_texto(self,cadena):\n",
        "    self.cadena=cadena\n",
        "    result=self.cadena.lower()\n",
        "    result=re.sub(r'[^\\w|\\s|@|$|€|%]','',result)\n",
        "    return result\n",
        "\n",
        "\n",
        "BoW=ClasificacionDocumentos()\n",
        "frase_final_1='A C@rlos le gustan los p€rr0s. A Ju@n también le gustan los p€rr0s, ¡¿pero prefiere gatos?!.'\n",
        "print(BoW.limpiar_texto(frase_final_1))"
      ],
      "metadata": {
        "id": "zubwoujY0B1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defbe19a-9124-4fe8-cd90-cae7dccbc278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a c@rlos le gustan los p€rr0s a ju@n también le gustan los p€rr0s pero prefiere gatos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJAfDO2Qkp6N",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "02994084-e981-4ec7-ad45-7419fe82954f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Correcto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#@title Comprueba el paso 1. Limpieza de texto.\n",
        "def check():\n",
        "  texto_check=\"Lorem Ipsum es simplemente el texto de relleno de las imprentas y archivos de texto. Lorem Ipsum ha sido el texto de relleno estándar de las industrias desde el año 1500, cuando un impresor (N. del T. persona que se dedica a la imprenta) desconocido usó una galería de textos y los mezcló de tal manera que logró hacer un libro de textos especimen. No sólo sobrevivió 500 años, sino que tambien ingresó como texto de relleno en documentos electrónicos, quedando esencialmente igual al original. Fue popularizado en los 60s con la creación de las hojas 'Letraset', las cuales contenian pasajes de Lorem Ipsum, y más recientemente con software de autoedición, como por ejemplo Aldus PageMaker, el cual incluye versiones de Lorem Ipsum.\"\n",
        "  if ClasificacionDocumentos().limpiar_texto(frase_final_1)=='a c@rlos le gustan los p€rr0s a ju@n también le gustan los p€rr0s pero prefiere gatos' and ClasificacionDocumentos().limpiar_texto('H@y que limpiar este texto, para poder avanzar. ¿Lo ha$ conseguido? ¿S€guro al 100%? ¡Enhorabuena!')=='h@y que limpiar este texto para poder avanzar lo ha$ conseguido s€guro al 100% enhorabuena' and ClasificacionDocumentos().limpiar_texto(texto_check)=='lorem ipsum es simplemente el texto de relleno de las imprentas y archivos de texto lorem ipsum ha sido el texto de relleno estándar de las industrias desde el año 1500 cuando un impresor n del t persona que se dedica a la imprenta desconocido usó una galería de textos y los mezcló de tal manera que logró hacer un libro de textos especimen no sólo sobrevivió 500 años sino que tambien ingresó como texto de relleno en documentos electrónicos quedando esencialmente igual al original fue popularizado en los 60s con la creación de las hojas letraset las cuales contenian pasajes de lorem ipsum y más recientemente con software de autoedición como por ejemplo aldus pagemaker el cual incluye versiones de lorem ipsum':\n",
        "    return 'Correcto'\n",
        "  else:\n",
        "    return 'Incorrecto'\n",
        "\n",
        "check()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2. Tokenizar.\n",
        "\n",
        "El anglicismo tokenizar es el acto de dividir las cadenas de texto en piezas más pequeñas como palabras, palabras clave ('inteligencia artificial') o incluso frases enteras si nos interesase saber su frecuencia de aparición ('los datos son el futuro').\n",
        "\n",
        "En este caso, vamos a hacer lo más simple que es dividir por palabras. **Recuerda que debes limpiar el texto antes.**\n",
        "\n",
        "\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "\n",
        "* Entrada\n",
        "\n",
        "      'A C@rlos le gustan los p€rr0s. A Ju@n también le gustan los p€rr0s, ¡¿pero prefiere gatos?!.'\n",
        "\n",
        "\n",
        "* Salida\n",
        "      ['a', 'c@rlos', 'le', 'gustan', 'los', 'p€rr0s', 'a', 'ju@n', 'también', 'le', 'gustan', 'los', 'p€rr0s', 'pero', 'prefiere', 'gatos']\n",
        "\n",
        "Como puedes observar, una simple frase se vuelve una lista con muchos elementos de los cuales una gran cantidad no aportan información (los artículos los, el pronombre le y la preposición a). A estas palabras que no aportan información se las llama Stopwords.\n",
        "\n",
        "Para evitar almacenar en base de datos y mejorar el rendimiento del código se suelen ignorar las Stopwords creando una lista con aquellas palabras que no deseamos prestar atención. \n",
        "\n",
        "Ejemplo con Stopwords ignoradas\n",
        "\n",
        "\n",
        "* Entrada\n",
        "\n",
        "      'A C@rlos le gustan los p€rr0s. A Ju@n también le gustan los p€rr0s, ¡¿pero prefiere gatos?!.'\n",
        "\n",
        "\n",
        "* Salida\n",
        "      ['c@rlos', 'gustan', 'p€rr0s', 'ju@n', 'también', 'gustan', 'p€rr0s', 'pero', 'prefiere', 'gatos']\n",
        "\n",
        "Ahora mismo la diferencia únicamente es pasar de una lista de 16 a una lista de 10. Pero imagínate que en vez de esta frase tuvieras un libro entero, ahí si que se notaría una gran diferencia.\n",
        "\n",
        "Dicho todo esto, ¡es tu turno!\n",
        "\n",
        "Modifica la clase creada en el ejercicio anterior para añadir el método tokenizar_por_palabras que reciba como parámetro una cadena de texto y la lista de Stopwords a ignorar y devuelva una lista con las palabras de la cadena de texto que no estén incluidas en la lista de Stopwords.\n",
        "\n",
        "En este caso la lista de Stopwords será: \n",
        "\n",
        "      ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']"
      ],
      "metadata": {
        "id": "LY9LxcMuSO3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Añade la función tokenizar_por_palabras\n",
        "\n",
        "import re\n",
        "\n",
        "class ClasificacionDocumentos:\n",
        "  \n",
        "  def limpiar_texto(self,cadena):\n",
        "    self.cadena=cadena\n",
        "    result=self.cadena.lower()\n",
        "    result=re.sub(r'[^\\w\\s|@|$|€|%]','',result)\n",
        "    return result\n",
        "\n",
        "  def tokenizar_por_palabras(self,texto,stopwords):\n",
        "    self.texto=texto\n",
        "    self.stopwords=stopwords\n",
        "    texto_clean=ClasificacionDocumentos().limpiar_texto(self.texto)\n",
        "    list_texto=texto_clean.split()\n",
        "    return [word for word in list_texto if word not in self.stopwords]\n",
        "\n",
        "\n",
        "BoW=ClasificacionDocumentos()\n",
        "frase_final_1='A C@rlos le gustan los p€rr0s. A Ju@n también le gustan los p€rr0s, ¡¿pero prefiere gatos?!.'\n",
        "ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']\n",
        "print(BoW.tokenizar_por_palabras(frase_final_1,ignorar))\n"
      ],
      "metadata": {
        "id": "udxle1fIZlNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846f0154-f510-4907-80a2-16589450db68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c@rlos', 'gustan', 'p€rr0s', 'ju@n', 'también', 'gustan', 'p€rr0s', 'pero', 'prefiere', 'gatos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comprueba el paso 2\n",
        "def check2():\n",
        "  texto_check2='el perro a cogido la pelota y los gatos han cogido a los ratones'\n",
        "  if ClasificacionDocumentos().tokenizar_por_palabras(frase_final_1,ignorar)==['c@rlos', 'gustan', 'p€rr0s', 'ju@n', 'también', 'gustan', 'p€rr0s', 'pero', 'prefiere', 'gatos'] and ClasificacionDocumentos().tokenizar_por_palabras(texto_check2,ignorar)==['perro', 'cogido', 'pelota', 'gatos', 'han', 'cogido', 'ratones']:\n",
        "    return 'Correcto'\n",
        "  else:\n",
        "    return 'Incorrecto'\n",
        "\n",
        "check2()"
      ],
      "metadata": {
        "id": "VOK7xW_KatP6",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "75eddc7c-1d37-4c32-e8a1-5ee615bdfae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Correcto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3. Generar el vocabulario.\n",
        "\n",
        "Una vez que tenemos un método que tokeniza por palabras un texto, debes añadir, a la misma clase, el método vocabulario que reciba una lista de cadenas de texto y la lista de stopwords como parámetros y devuelva una lista de palabras únicas que aportan información.\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "* Entrada\n",
        "\n",
        "      textos=['El nombre de mi perro es Eevee', 'El nombre de mi gato es Calcetines y su apodo michi', 'El perro caza al gato, como el gato caza al ratón']\n",
        "\n",
        "      ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']\n",
        "\n",
        "      vocabulario(textos,ignorar)\n",
        "\n",
        "* Salida\n",
        "\n",
        "      ['nombre', 'mi', 'perro', 'es', 'eevee', 'gato', 'calcetines', 'su', 'apodo', 'michi', 'caza', 'como', 'ratón']\n",
        "\n"
      ],
      "metadata": {
        "id": "jwKQDDo9ilYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Añade tu función vocabulario\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class ClasificacionDocumentos:\n",
        "  \n",
        "  def limpiar_texto(self,cadena):\n",
        "    self.cadena=cadena\n",
        "    result=self.cadena.lower()\n",
        "    result=re.sub(r'[^\\w\\s|@|$|€|%]','',result)\n",
        "    return result\n",
        "\n",
        "  def tokenizar_por_palabras(self,texto,stopwords):\n",
        "    self.texto=texto\n",
        "    self.stopwords=stopwords\n",
        "    texto_clean=ClasificacionDocumentos().limpiar_texto(self.texto)\n",
        "    list_texto=texto_clean.split()\n",
        "    return [word for word in list_texto if word not in self.stopwords]\n",
        "\n",
        "  def vocabulario(self,lista_textos,stopwords):\n",
        "    self.lista_textos=lista_textos\n",
        "    self.stopwords=stopwords\n",
        "    lista_textos_clean=[ClasificacionDocumentos().limpiar_texto(frase) for frase in self.lista_textos]\n",
        "    tokenizar=[ClasificacionDocumentos().tokenizar_por_palabras(frase,self.stopwords) for frase in lista_textos_clean]\n",
        "    result=[]\n",
        "    for sentence in tokenizar:\n",
        "      for word in sentence:\n",
        "        if word not in result:\n",
        "          result.append(word)\n",
        "    return result\n",
        "\n",
        "  \n",
        "\n",
        "BoW=ClasificacionDocumentos()\n",
        "lista_frases_1=['el nombre de mi perro es Eevee', 'el nombre de mi gato es Calcetines y su apodo michi', 'el perro caza al gato como el gato caza al ratón']\n",
        "ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']\n",
        "print(BoW.vocabulario(lista_frases_1,ignorar))\n"
      ],
      "metadata": {
        "id": "0h-JRkpBlbny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72702f4-d10f-4ae3-ae68-8ad13dfaab83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nombre', 'mi', 'perro', 'es', 'eevee', 'gato', 'calcetines', 'su', 'apodo', 'michi', 'caza', 'como', 'ratón']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comprueba el paso 3\n",
        "lista_frases_check3=['Bitcoin no logra superar la resistencia y deja los USD 40,000 fuera del alcance al cierre semanal','Precio de Bitcoin en Latinoamérica cierra una semana de altibajos con leve inclinación a la baja','70 años de cárcel enfrentaría estafador que robó USD 2000 millones en bitcoin']\n",
        "\n",
        "def check3():\n",
        "  if ClasificacionDocumentos().vocabulario(lista_frases_1,ignorar)==['nombre', 'mi', 'perro', 'es', 'eevee', 'gato', 'calcetines', 'su', 'apodo', 'michi', 'caza', 'como', 'ratón'] and ClasificacionDocumentos().vocabulario(lista_frases_check3,ignorar)==['bitcoin', 'no', 'logra', 'superar', 'resistencia', 'deja', 'usd', '40000', 'fuera', 'del', 'alcance', 'cierre', 'semanal', 'precio', 'en', 'latinoamérica', 'cierra', 'una', 'semana', 'altibajos', 'con', 'leve', 'inclinación', 'baja', '70', 'años', 'cárcel', 'enfrentaría', 'estafador', 'que', 'robó', '2000', 'millones']:\n",
        "    return 'Correcto'\n",
        "  else:\n",
        "    return 'Incorrecto'\n",
        "check3()"
      ],
      "metadata": {
        "id": "S_zPMI3TnzG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1adc2938-04e2-4b5f-a40f-6d170aabaedd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Correcto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paso 4. Generación de vectores\n",
        "\n",
        "En este paso debemos crear el método vector_frecuencia que, al introducir como párametros una frase y una lista de palabras (vocabulario), nos devuelva la frecuencia con la que aparece cada palabra de la frase en el vocabulario.\n",
        "\n",
        "La frecuencia con la que aparece cada palabra se representará como una lista de valores, de la misma longitud del vocabulario, que por defecto tomará el valor 0 y se aumentará en 1 cada vez que aparezca dicha palabra.\n",
        "\n",
        "\n",
        "\n",
        "Si la entrada es:\n",
        "\n",
        "      frase='Eevee es un pokemon que parece un perro y Meowth un gato'\n",
        "\n",
        "      vocabulario=['nombre', 'mi', 'perro', 'es', 'eevee', 'gato', 'Calcetines', 'su', 'apodo', 'michi', 'caza', 'como', 'ratón']\n",
        "\n",
        "      ClasificacionDocumentos().vector_frecuencia(frase,vocabulario)\n",
        "\n",
        "La salida es:\n",
        "\n",
        "      [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "      \n"
      ],
      "metadata": {
        "id": "p3ceGOwnpMCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Añade la función vectores_frecuencia\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BoW=ClasificacionDocumentos()\n",
        "\n",
        "frase_4='Eevee es un pokemon que parece un perro y Meowth un gato'\n",
        "lista_frases_1=['el nombre de mi perro es Eevee', 'el nombre de mi gato es Calcetines y su apodo michi', 'el perro caza al gato como el gato caza al ratón']\n",
        "ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']\n",
        "vocab=BoW.vocabulario(lista_frases_1,ignorar)\n",
        "print(vocab)\n",
        "print(BoW.vector_frecuencia(frase_4,vocab))\n"
      ],
      "metadata": {
        "id": "TdMMrloarylt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comprueba el paso 4. Vector de frecuencia\n",
        "lista_frases_check3=['Bitcoin no logra superar la resistencia y deja los USD 40,000 fuera del alcance al cierre semanal','Precio de Bitcoin en Latinoamérica cierra una semana de altibajos con leve inclinación a la baja','70 años de cárcel enfrentaría estafador que robó USD 2000 millones en bitcoin']\n",
        "frase_check4='El bitcoin es la criptomoneda más famosa pero cuesta muchos USD para Latinoamérica'\n",
        "def check4():\n",
        "  if ClasificacionDocumentos().vector_frecuencia(frase_4,vocab)==[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0] and ClasificacionDocumentos().vector_frecuencia(frase_check4,ClasificacionDocumentos().vocabulario(lista_frases_check3,ignorar))==[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:\n",
        "    return 'Correcto'\n",
        "  else:\n",
        "    return 'Incorrecto'\n",
        "check4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g4DGY0ntqTdz",
        "outputId": "b29a7d71-fe0f-4aab-fb46-c3b49879d771",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Correcto'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paso 5. Modelo BagOfWords\n",
        "\n",
        "Una vez creados los métodos necesarios y comprobados que funcionan correctamente es el momento de crear un 5º método que realice todo el proceso de forma autómata.\n",
        "\n",
        "Dicho método se llamará completo y tendrá 3 parámetros (sin contar el self):\n",
        "\n",
        "* La lista de frases de la que se desea conocer sus vectores de frecuencia.\n",
        "\n",
        "* La lista de frases con la que se construirá el vocabulario con las palabras que se desea conocer la frecuencia.\n",
        "\n",
        "* La lista de stopwords que deseas que no formen parte del vocabulario.\n",
        "\n",
        "Este método devolverá la lista con los vectores de frecuencia de la primera lista de frases.\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "      lista_frase=['¿Perro o gato?', 'El ratón teme al gato', 'Gato, Perro, Gato, Gato, Perro']\n",
        "      textos_ejemplo4=['el nombre de mi perro es Eevee', 'el nombre de mi gato es Calcetines y su apodo michi', 'el perro caza al gato como el gato caza al ratón', 'mi nombre es Nodd3r']\n",
        "      ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']\n",
        "      completo(lista_frases,textos_ejemplo4,ignorar)\n",
        "\n",
        "\n",
        "la salida será:\n",
        "\n",
        "      Los vectores de frecuencia son:\n",
        "      [[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0]]\n"
      ],
      "metadata": {
        "id": "lG6FqaG15Gd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define la clase ClasificacionDocumentos\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BoW=ClasificacionDocumentos()\n",
        "\n",
        "lista_frases_4=['¿Perro o gato?', 'El ratón teme al gato', 'Gato, Perro, Gato, Gato, Perro']\n",
        "textos_ejemplo_4=['el nombre de mi perro es Eevee', 'el nombre de mi gato es Calcetines y su apodo michi', 'el perro caza al gato como el gato caza al ratón']\n",
        "ignorar = ['el','la','las','los','le','lo','a', 'y', 'o', 'de', 'al']\n",
        "\n",
        "print(BoW.completo(lista_frases_4,textos_ejemplo_4,ignorar))\n"
      ],
      "metadata": {
        "id": "-Qyv7rJU7iIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comprueba el 5 paso.\n",
        "\n",
        "def check5():\n",
        "  if ClasificacionDocumentos().completo(lista_frases_4,textos_ejemplo_4,ignorar)=='Los vectores de frecuencia son:\\n[[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0]]':\n",
        "    return 'Correcto'\n",
        "  else:\n",
        "    return 'Incorrecto'\n",
        "check5()"
      ],
      "metadata": {
        "id": "9mJCAkCnsQT1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import Javascript\n",
        "#@title Consigue el Token para corregir el proyecto en Nodd3r\n",
        "\n",
        "frases_token=['Python es el lenguaje de programación de moda', 'Javascript, Python, Data Scientist', 'Un data scientist no tiene que saber jascript pero sí python']\n",
        "lista_frases_token=['Python, JavaScript y PHP son lenguajes de programación','Para ser Data Scientist es importante saber programación', 'Data Scientist es la profesión de moda', 'La programación de la TV es muy mala','Programación, programación, PROGRAMACIÓN']\n",
        "ignorar_token=['es','el','de','y','son','de','ser']\n",
        "\n",
        "import hashlib\n",
        "\n",
        "pwd = hashlib.sha256((str(print(ClasificacionDocumentos().completo(frases_token,lista_frases_token,ignorar_token)))+str(print(ClasificacionDocumentos().limpiar_texto('¿H@la!')))+str(print(ClasificacionDocumentos().vector_frecuencia('Rojo, Azul, Amarillo',['rojo','amarillo'])))).encode())\n",
        "#print('El token es:\\n',pwd.hexdigest())\n",
        "if pwd.hexdigest()[0:6] == 'd5c4d2':\n",
        "  print('¡Felicidades! puedes avanzar al siguiente modulo \\n El token es: ',pwd.hexdigest())\n",
        "else:\n",
        "  print('Hay algún error en el código o tu forma es diferente a la planteada, pregunta por el foro si no lo ves claro.')\n"
      ],
      "metadata": {
        "id": "sUNXsRbYtBpr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}